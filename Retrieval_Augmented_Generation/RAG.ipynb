{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1: Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from functions import *\n",
    "\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "from torch import cat, save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the data\n",
    "# Extract text and images\n",
    "filename_list = [\"raw/\"+f for f in os.listdir('raw')]\n",
    "\n",
    "text_content_list = []\n",
    "image_content_list = []\n",
    "for filename in filename_list:\n",
    "\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "\n",
    "    text_content_list.extend(parse_html_content(html_content))\n",
    "    image_content_list.extend(parse_html_images(html_content))\n",
    "\n",
    "print(len(text_content_list))\n",
    "print(len(image_content_list))\n",
    "\n",
    "text_list = []\n",
    "for content in text_content_list:\n",
    "    # concatenate title and section header\n",
    "    section = content['section'] + \": \"\n",
    "    # append text from paragraph to fill CLIP's 256 sequence limit\n",
    "    text = section + content['text'][:256-len(section)]\n",
    "    \n",
    "    text_list.append(text)\n",
    "\n",
    "image_list = []\n",
    "for content in image_content_list:\n",
    "    image_list.append(Image.open(content['image_path']))\n",
    "\n",
    "print(len(text_list))\n",
    "print(len(image_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute embeddings using CLIP\n",
    "# import model\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "# import processor (handles text tokenization and image preprocessing)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\") \n",
    "# pre-process text and images\n",
    "inputs = processor(text=text_list, images=image_list, return_tensors=\"pt\", padding=True)\n",
    "# compute embeddings with CLIP\n",
    "outputs = model(**inputs)\n",
    "# store embeddings in single torch tensor\n",
    "text_embeddings = outputs.text_embeds\n",
    "image_embeddings = outputs.image_embeds\n",
    "print(text_embeddings.shape)\n",
    "print(image_embeddings.shape)\n",
    "# Save Data\n",
    "# save content list as JSON\n",
    "save_to_json(text_content_list, output_file='data/text_content.json')\n",
    "save_to_json(image_content_list, output_file='data/image_content.json')\n",
    "# save embeddings to file\n",
    "save(text_embeddings, 'data/text_embeddings.pt')\n",
    "save(image_embeddings, 'data/image_embeddings.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Multimodal Article Question Answering Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from functions import *\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torch import load, matmul, argsort\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load article contents\n",
    "text_content_list = load_from_json('data/text_content.json')\n",
    "image_content_list = load_from_json('data/image_content.json')\n",
    "\n",
    "# load embeddings\n",
    "text_embeddings = load('data/text_embeddings.pt', weights_only=True)\n",
    "image_embeddings = load('data/image_embeddings.pt', weights_only=True)\n",
    "\n",
    "print(text_embeddings.shape)\n",
    "print(image_embeddings.shape)\n",
    "\n",
    "print(text_content_list[4]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query\n",
    "query = \"What is CLIP's contrastive loss function?\"\n",
    "# query = \"What are the three paths described for making LLMs multimodal?\"\n",
    "# query = \"What is an intuitive explanation of multimodal embeddings?\"\n",
    "\n",
    "# embed query\n",
    "query_embed = embed_text(query)\n",
    "print(query_embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Multimodal search\n",
    "k = 5\n",
    "threshold = 0.1\n",
    "\n",
    "# multimodal search over articles\n",
    "text_similarities = matmul(query_embed, text_embeddings.T)\n",
    "image_similarities = matmul(query_embed, image_embeddings.T)\n",
    "\n",
    "# rescale similarities via softmax\n",
    "temp=0.25\n",
    "text_scores = softmax(text_similarities/temp, dim=1)\n",
    "image_scores = softmax(image_similarities/temp, dim=1)\n",
    "\n",
    "# return top k filtered text results\n",
    "isorted_scores = argsort(text_scores, descending=True)[0]\n",
    "sorted_scores = text_scores[0][isorted_scores]\n",
    "\n",
    "itop_k_filtered = [idx.item() for idx, score in zip(isorted_scores, sorted_scores) if score.item() >= threshold][:k]\n",
    "top_k = [text_content_list[i] for i in itop_k_filtered]\n",
    "\n",
    "top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## text and image search\n",
    "text_results, text_scores = similarity_search(query_embed, text_embeddings, text_content_list, k=15, threshold=0.01, temperature=0.25)\n",
    "image_results, image_scores = similarity_search(query_embed, image_embeddings, image_content_list, k=5, threshold=0.25, temperature=0.5)\n",
    "\n",
    "i=1\n",
    "for text in text_results:\n",
    "    if text_results:\n",
    "        print(i, \"-\", text['text'])\n",
    "        i=i+1\n",
    "for image in image_results:\n",
    "    display(Image(filename=image['image_path']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Engineering\n",
    "# to make the prompt ready to use in LLM\n",
    "\n",
    "#format context\n",
    "text_context = \"\"\n",
    "for text in text_results:\n",
    "    if text_results:\n",
    "        text_context = text_context + \"**Article title:** \" + text['article_title'] + \"\\n\"\n",
    "        text_context = text_context + \"**Section:**  \" + text['section'] + \"\\n\"\n",
    "        text_context = text_context + \"**Snippet:** \" + text['text'] + \"\\n\\n\"\n",
    "image_context = \"\"\n",
    "for image in image_results:\n",
    "    if image_results:\n",
    "        image_context = image_context + \"**Article title:** \" + image['article_title'] + \"\\n\"\n",
    "        image_context = image_context + \"**Section:**  \" + image['section'] + \"\\n\"\n",
    "        image_context = image_context + \"**Image Path:**  \" + image['image_path'] + \"\\n\"\n",
    "        image_context = image_context + \"**Image Caption:** \" + image['caption'] + \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt construction\n",
    "# construct prompt template\n",
    "prompt = f\"\"\"Given the query \"{query}\" and the following relevant snippets:\n",
    "\n",
    "{text_context}\n",
    "{image_context}\n",
    "\n",
    "Please provide a concise and accurate answer to the query, incorporating relevant information from the provided snippets where available.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt LLM\n",
    "\n",
    "ollama.pull('llama3.2-vision')\n",
    "response = ollama.chat(\n",
    "    model='llama3.2-vision',\n",
    "    messages=[{\n",
    "        'role': 'user',\n",
    "        'content': prompt,\n",
    "        'images': [image[\"image_path\"] for image in image_results]\n",
    "    }]\n",
    ")\n",
    "\n",
    "print(response['message']['content'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
