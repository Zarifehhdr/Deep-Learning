{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLaMA 3.2 Vision for Image-based Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ollama in /Users/zah47/anaconda3/lib/python3.10/site-packages (0.4.8)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in /Users/zah47/anaconda3/lib/python3.10/site-packages (from ollama) (2.10.6)\n",
      "Requirement already satisfied: httpx<0.29,>=0.27 in /Users/zah47/anaconda3/lib/python3.10/site-packages (from ollama) (0.28.1)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/zah47/anaconda3/lib/python3.10/site-packages (from httpx<0.29,>=0.27->ollama) (1.0.9)\n",
      "Requirement already satisfied: certifi in /Users/zah47/anaconda3/lib/python3.10/site-packages (from httpx<0.29,>=0.27->ollama) (2025.1.31)\n",
      "Requirement already satisfied: anyio in /Users/zah47/anaconda3/lib/python3.10/site-packages (from httpx<0.29,>=0.27->ollama) (4.9.0)\n",
      "Requirement already satisfied: idna in /Users/zah47/anaconda3/lib/python3.10/site-packages (from httpx<0.29,>=0.27->ollama) (3.4)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/zah47/anaconda3/lib/python3.10/site-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /Users/zah47/anaconda3/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (4.13.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/zah47/anaconda3/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Users/zah47/anaconda3/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (2.27.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/zah47/anaconda3/lib/python3.10/site-packages (from anyio->httpx<0.29,>=0.27->ollama) (1.2.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/zah47/anaconda3/lib/python3.10/site-packages (from anyio->httpx<0.29,>=0.27->ollama) (1.2.2)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProgressResponse(status='success', completed=None, total=None, digest=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ollama\n",
    "ollama.pull(\"llama3.2-vision\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image shows a cat and a dog standing in a field of yellow flowers.\n"
     ]
    }
   ],
   "source": [
    "# pass image and question to the model\n",
    "response = ollama.chat(\n",
    "    model = \"llama3.2-vision\",\n",
    "    messages= [{\n",
    "        'role': 'user',\n",
    "        'content': 'What is in the image?',\n",
    "        'images': ['../pets.webp']\n",
    "    }]\n",
    ")\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Question Answering (Streaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image shows a cat and dog standing in a field of yellow flowers. The cat is on the left side of the image, facing forward. It has brown and grey stripes with a fluffy tail that is curled up over its back. To the right of the cat is a dog with brown, grey, and white fur. It is facing forward and appears to be smiling. Both animals are standing in a field of yellow flowers with green grass. The background is blurry but appears to be a body of water behind the field of flowers."
     ]
    }
   ],
   "source": [
    "# pass image and question to the model\n",
    "stream = ollama.chat(\n",
    "    model = \"llama3.2-vision\",\n",
    "    messages= [{\n",
    "        'role': 'user',\n",
    "        'content': 'What is in the image?',\n",
    "        'images': ['../pets.webp']\n",
    "    }],\n",
    "    stream= True,\n",
    ")\n",
    "for chunk in stream:\n",
    " print(chunk['message']['content'], end= '', flush = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaining meme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This meme is a play on the \"You have a meme idea\" and \"You forget it\" meme format. The top panel features a cartoon frog with a wide-eyed, toothy grin, while the bottom panel shows the same frog with a confused expression. The text reads, \"You have a meme idea\" in the top panel and \"You forget it\" in the bottom panel.\n",
      "\n",
      "The meme is humorous because it pokes fun at the common experience of having a great idea for a meme, only to forget it moments later. The use of a cartoon frog as the character adds to the humor, as it's a relatable and endearing character that many people can identify with. Overall, the meme is a lighthearted way to poke fun at the fleeting nature of creativity and the tendency to forget good ideas."
     ]
    }
   ],
   "source": [
    "# pass image and question to the model\n",
    "stream = ollama.chat(\n",
    "    model = \"llama3.2-vision\",\n",
    "    messages= [{\n",
    "        'role': 'user',\n",
    "        'content': 'Explain this meme to me',\n",
    "        'images': ['../meme.webp']\n",
    "    }],\n",
    "    stream= True,\n",
    ")\n",
    "for chunk in stream:\n",
    " print(chunk['message']['content'], end= '', flush = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The flow of method is as follows:\n",
      "\n",
      "1. **Load taxonomy**: Define files and data for extraction.\n",
      "2. **Digitize**: Use OCR to detect text and its location.\n",
      "3. **Classify**: Classify the documents from the specified list.\n",
      "4. **Extract**: Extract information from the documents.\n",
      "5. **Validate**: If needed, a human can confirm the extracted data.\n",
      "6. **Export**: Export the extracted information for further use.\n",
      "\n",
      "This flowchart provides a step-by-step guide to the method's process."
     ]
    }
   ],
   "source": [
    "# pass image and question to the model\n",
    "stream = ollama.chat(\n",
    "    model = \"llama3.2-vision\",\n",
    "    messages= [{\n",
    "        'role': 'user',\n",
    "        'content': 'Explain the flow of method.',\n",
    "        'images': ['../doc.png']\n",
    "    }],\n",
    "    stream= True,\n",
    ")\n",
    "for chunk in stream:\n",
    " print(chunk['message']['content'], end= '', flush = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 1 Description:\n",
      "\n",
      "The image depicts a white ram with a distinctive curved horn on its head, standing on a rocky terrain with its head turned to the left. The ram's thick, fluffy coat is a uniform white, and its horn is brown and curved in a spiral shape. Its head is turned to the left, and its eyes are not visible. The ram's body is facing to the right, and its legs are spread out, with its left leg slightly bent.\n",
      "\n",
      "In the background, there are dark branches and a rocky terrain, suggesting that the ram is in a natural environment, possibly a mountainous or rocky area. The overall atmosphere of the image is one of serenity and tranquility, with the ram seemingly at ease in its surroundings.\n",
      "\n",
      "Image 2 Description:\n",
      "\n",
      "The image depicts a young calf standing in a grassy field, its head held high and its ears perked up. The calf's fur is a mix of black and white, with its face and ears being black and its body being white. A blue and purple rope is tied around its head, possibly for training or to keep it in a specific area.\n",
      "\n",
      "In the background, a dirt path can be seen leading into the distance, with trees and a blue sky visible beyond. The overall atmosphere of the image suggests that the calf is being kept in a controlled environment, possibly on a farm or ranch, where it is being trained or cared for."
     ]
    }
   ],
   "source": [
    "# First image\n",
    "description1 = \"\"\n",
    "stream1 = ollama.chat(\n",
    "    model=\"llama3.2-vision\",\n",
    "    messages=[{\n",
    "        'role': 'user',\n",
    "        'content': 'Describe this image.',\n",
    "        'images': [\"../img1.jpg\"]\n",
    "    }],\n",
    "    stream=True,\n",
    ")\n",
    "print(\"Image 1 Description:\\n\")\n",
    "for chunk in stream1:\n",
    "    content = chunk['message']['content']\n",
    "    description1 += content\n",
    "    print(content, end='', flush=True)\n",
    "\n",
    "# Second image\n",
    "description2 = \"\"\n",
    "stream2 = ollama.chat(\n",
    "    model=\"llama3.2-vision\",\n",
    "    messages=[{\n",
    "        'role': 'user',\n",
    "        'content': 'Describe this image.',\n",
    "        'images': [\"../img2.webp\"]\n",
    "    }],\n",
    "    stream=True,\n",
    ")\n",
    "print(\"\\n\\nImage 2 Description:\\n\")\n",
    "for chunk in stream2:\n",
    "    content = chunk['message']['content']\n",
    "    description2 += content\n",
    "    print(content, end='', flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Comparison Result:\n",
      "\n",
      "Here are the similarities and differences between the two image descriptions:\n",
      "\n",
      "**Similarities:**\n",
      "\n",
      "* Both descriptions mention the animal's head and body orientation, with the ram's head turned to the left and the calf's head held high.\n",
      "* Both descriptions mention the background environment, with the ram's image featuring a rocky terrain and the calf's image featuring a grassy field.\n",
      "* Both descriptions convey a sense of atmosphere, with the ram's image being serene and tranquil and the calf's image suggesting a controlled environment.\n",
      "\n",
      "**Differences:**\n",
      "\n",
      "* **Animal type**: The most obvious difference is that the first image describes a ram, while the second image describes a calf.\n",
      "* **Color and coat**: The ram has a uniform white coat with a brown curved horn, while the calf has a mix of black and white fur.\n",
      "* **Environment**: The ram is in a natural, rocky environment, while the calf is in a controlled, grassy environment with a dirt path.\n",
      "* **Accessories**: The calf has a rope tied around its head, which is not present in the ram's image.\n",
      "* **Mood and atmosphere**: The ram's image conveys a sense of serenity and tranquility, while the calf's image suggests a more controlled or trained environment.\n"
     ]
    }
   ],
   "source": [
    "# Compare descriptions using ollama.chat\n",
    "comparison = ollama.chat(\n",
    "    model=\"llama3.2-vision\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"Compare the following two image descriptions and highlight their similarities and differences:\n",
    "\n",
    "Description 1:\n",
    "{description1}\n",
    "\n",
    "Description 2:\n",
    "{description2}\n",
    "\"\"\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\n\\nComparison Result:\\n\")\n",
    "print(comparison['message']['content'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
